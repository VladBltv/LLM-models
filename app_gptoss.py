from fastapi import FastAPI
from pydantic import BaseModel
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer
import uvicorn
import os


MODEL_NAME = "TeichAI/gpt-oss-20b-claude-4.5-sonnet-high-reasoning-distill"

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
gpu_memory_util = float(os.environ.get("GPU_MEMORY_UTILIZATION", "0.75"))
max_model_length = int(os.environ.get("MAX_MODEL_LEN", "4096"))  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏

print(f"üîß –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏:")
print(f"   GPU Memory Utilization: {gpu_memory_util}")
print(f"   Max Model Length: {max_model_length}")

try:
    llm = LLM(
        model=MODEL_NAME,
        tensor_parallel_size=1,
        gpu_memory_utilization=gpu_memory_util,
        max_model_len=max_model_length,  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏ KV cache
        enforce_eager=False,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∂–∏–º
    )
except ValueError as e:
    if "max seq len" in str(e).lower() or "kv cache" in str(e).lower():
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Å max_model_len={max_model_length}, –ø—Ä–æ–±—É—é —É–º–µ–Ω—å—à–∏—Ç—å –¥–æ 2048...")
        max_model_length = 2048
        llm = LLM(
            model=MODEL_NAME,
            tensor_parallel_size=1,
            gpu_memory_utilization=0.7,  # –ï—â–µ –º–µ–Ω—å—à–µ
            max_model_len=2048,
            enforce_eager=False,
        )
    else:
        raise

app = FastAPI(title="GPT-OSS-20B-Claude-4.5-Sonnet-High-Reasoning-Distill API")


class GenerateRequest(BaseModel):
    prompt: str
    temperature: float = 0.3


@app.post("/generate_gptoss")
async def generate_gptoss(request: GenerateRequest):
    messages = [{"role": "user", "content": request.prompt}]
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º chat template
    prompt_text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )
    
    sampling_params = SamplingParams(
        temperature=request.temperature,
        top_p=0.9,
        top_k=50,
        max_tokens=1024,
    )

    outputs = llm.generate(
        prompt_text,
        use_tqdm=False,
        sampling_params=sampling_params,
    )

    response_text = outputs[0].outputs[0].text.strip()
    return {"response": response_text}


if __name__ == "__main__":
    host = os.environ.get("HOST", "0.0.0.0")
    port = int(os.environ.get("PORT", "8084"))
    uvicorn.run(app, host=host, port=port)

