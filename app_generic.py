from fastapi import FastAPI
from pydantic import BaseModel
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer
import uvicorn
import os


# –ü–æ–ª—É—á–∞–µ–º –∏–º—è –º–æ–¥–µ–ª–∏ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)
MODEL_NAME = os.environ.get("MODEL_NAME")
if not MODEL_NAME:
    raise ValueError("MODEL_NAME environment variable is required!")

# –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
gpu_memory_util = float(os.environ.get("GPU_MEMORY_UTILIZATION", "0.85"))
max_model_length = int(os.environ.get("MAX_MODEL_LEN", "4096"))
trust_remote_code = os.environ.get("TRUST_REMOTE_CODE", "false").lower() == "true"
endpoint_name = os.environ.get("ENDPOINT_NAME", "generate")
api_title = os.environ.get("API_TITLE", f"{MODEL_NAME.split('/')[-1]} API")

print(f"üîß –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {MODEL_NAME}")
print(f"üîß –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–≥—Ä—É–∑–∫–∏:")
print(f"   GPU Memory Utilization: {gpu_memory_util}")
print(f"   Max Model Length: {max_model_length}")
print(f"   Trust Remote Code: {trust_remote_code}")
print(f"   Endpoint: /{endpoint_name}")

# –ó–∞–≥—Ä—É–∂–∞–µ–º tokenizer
try:
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_NAME,
        trust_remote_code=trust_remote_code
    )
except Exception as e:
    print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ tokenizer: {e}")
    print("üí° –ü—Ä–æ–±—É—é —Å trust_remote_code=True...")
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_NAME,
        trust_remote_code=True
    )
    trust_remote_code = True

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
try:
    llm = LLM(
        model=MODEL_NAME,
        tensor_parallel_size=1,
        gpu_memory_utilization=gpu_memory_util,
        max_model_len=max_model_length,
        enforce_eager=False,
        trust_remote_code=trust_remote_code,
    )
except ValueError as e:
    if "max seq len" in str(e).lower() or "kv cache" in str(e).lower():
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Å max_model_len={max_model_length}, –ø—Ä–æ–±—É—é —É–º–µ–Ω—å—à–∏—Ç—å –¥–æ 2048...")
        max_model_length = 2048
        llm = LLM(
            model=MODEL_NAME,
            tensor_parallel_size=1,
            gpu_memory_utilization=0.7,
            max_model_len=2048,
            enforce_eager=False,
            trust_remote_code=trust_remote_code,
        )
    else:
        raise
except Exception as e:
    if "model type" in str(e).lower() or "architecture" in str(e).lower():
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        if not trust_remote_code:
            print("üí° –ü—Ä–æ–±—É—é —Å trust_remote_code=True...")
            try:
                llm = LLM(
                    model=MODEL_NAME,
                    tensor_parallel_size=1,
                    gpu_memory_utilization=gpu_memory_util,
                    max_model_len=max_model_length,
                    enforce_eager=False,
                    trust_remote_code=True,
                )
            except Exception as e2:
                print(f"‚ùå –û—à–∏–±–∫–∞ –¥–∞–∂–µ —Å trust_remote_code=True: {e2}")
                raise
        else:
            raise
    else:
        raise

app = FastAPI(title=api_title)


class GenerateRequest(BaseModel):
    prompt: str
    temperature: float = 0.3
    max_tokens: int = 1024


@app.post(f"/{endpoint_name}")
async def generate(request: GenerateRequest):
    messages = [{"role": "user", "content": request.prompt}]
    
    # –ü—Ä–æ–±—É–µ–º –ø—Ä–∏–º–µ–Ω–∏—Ç—å chat template
    try:
        prompt_text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )
    except Exception:
        # –ï—Å–ª–∏ chat template –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ prompt
        prompt_text = request.prompt
    
    sampling_params = SamplingParams(
        temperature=request.temperature,
        top_p=0.9,
        top_k=50,
        max_tokens=request.max_tokens,
    )

    outputs = llm.generate(
        prompt_text,
        use_tqdm=False,
        sampling_params=sampling_params,
    )

    # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç
    if hasattr(outputs[0].outputs[0], 'text'):
        response_text = outputs[0].outputs[0].text.strip()
    else:
        response_text = tokenizer.decode(
            outputs[0].outputs[0].token_ids,
            skip_special_tokens=True
        ).strip()
    
    return {"response": response_text}


if __name__ == "__main__":
    host = os.environ.get("HOST", "0.0.0.0")
    port = int(os.environ.get("PORT", "8080"))
    uvicorn.run(app, host=host, port=port)

