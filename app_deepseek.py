import os
import sys

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è transformers
os.environ["TRUST_REMOTE_CODE"] = "true"

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –æ–±–Ω–æ–≤–ª—è–µ–º transformers –î–û –ª—é–±—ã—Ö –∏–º–ø–æ—Ä—Ç–æ–≤
print("üîß –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–µ—Ä—Å–∏–∏ transformers...")
try:
    import subprocess
    # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â—É—é –≤–µ—Ä—Å–∏—é
    result = subprocess.run(
        [sys.executable, "-m", "pip", "show", "transformers"],
        capture_output=True,
        text=True,
        timeout=30
    )
    current_version = None
    if result.returncode == 0:
        for line in result.stdout.split('\n'):
            if line.startswith('Version:'):
                current_version = line.split(':')[1].strip()
                break
    print(f"   –¢–µ–∫—É—â–∞—è –≤–µ—Ä—Å–∏—è: {current_version or '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞'}")
    
    # –û–±–Ω–æ–≤–ª—è–µ–º transformers –¥–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π –≤–µ—Ä—Å–∏–∏ (–ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ)
    print("üîß –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ transformers –¥–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π –≤–µ—Ä—Å–∏–∏...")
    result = subprocess.run(
        [sys.executable, "-m", "pip", "install", "--upgrade", "--force-reinstall", "transformers>=4.40.0", "--no-cache-dir"],
        timeout=300,
        capture_output=True,
        text=True
    )
    if result.returncode == 0:
        print("‚úÖ transformers –æ–±–Ω–æ–≤–ª–µ–Ω")
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–æ–≤—É—é –≤–µ—Ä—Å–∏—é
        result2 = subprocess.run(
            [sys.executable, "-m", "pip", "show", "transformers"],
            capture_output=True,
            text=True,
            timeout=30
        )
        if result2.returncode == 0:
            for line in result2.stdout.split('\n'):
                if line.startswith('Version:'):
                    new_version = line.split(':')[1].strip()
                    print(f"   –ù–æ–≤–∞—è –≤–µ—Ä—Å–∏—è: {new_version}")
                    break
    else:
        print(f"‚ö†Ô∏è  transformers –Ω–µ –æ–±–Ω–æ–≤–ª–µ–Ω")
        print(f"   –û—à–∏–±–∫–∞: {result.stderr[:200]}")
        print("üí° –ü—Ä–æ–¥–æ–ª–∂–∞—é —Å —Ç–µ–∫—É—â–µ–π –≤–µ—Ä—Å–∏–µ–π...")
except Exception as e:
    print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏/–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è transformers: {e}")
    print("üí° –ü—Ä–æ–¥–æ–ª–∂–∞—é —Å —Ç–µ–∫—É—â–µ–π –≤–µ—Ä—Å–∏–µ–π...")

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –ü–û–°–õ–ï –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
from fastapi import FastAPI
from pydantic import BaseModel

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º transformers –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤–µ—Ä—Å–∏—é
print("üîß –ò–º–ø–æ—Ä—Ç transformers...")
try:
    import transformers
    transformers_version = transformers.__version__
    print(f"   –í–µ—Ä—Å–∏—è transformers –≤ Python: {transformers_version}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ª–∏ –Ω–æ–≤–∞—è –≤–µ—Ä—Å–∏—è (qwen3 —Ç—Ä–µ–±—É–µ—Ç transformers >= 4.40.0)
    version_parts = transformers_version.split('.')
    major = int(version_parts[0]) if len(version_parts) > 0 and version_parts[0].isdigit() else 0
    minor = int(version_parts[1]) if len(version_parts) > 1 and version_parts[1].isdigit() else 0
    
    if major < 4 or (major == 4 and minor < 40):
        print(f"‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –í–µ—Ä—Å–∏—è transformers {transformers_version} —Å–ª–∏—à–∫–æ–º —Å—Ç–∞—Ä–∞—è –¥–ª—è qwen3")
        print("üí° –¢—Ä–µ–±—É–µ—Ç—Å—è transformers >= 4.40.0")
        print("üí° –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ...")
        result = subprocess.run(
            [sys.executable, "-m", "pip", "install", "--upgrade", "--force-reinstall", "transformers>=4.40.0", "--no-cache-dir"],
            timeout=300,
            capture_output=True,
            text=True
        )
        if result.returncode == 0:
            # –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥—É–ª—å
            import importlib
            importlib.reload(transformers)
            transformers_version = transformers.__version__
            print(f"   ‚úÖ –ü–æ—Å–ª–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {transformers_version}")
        else:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {result.stderr[:300]}")
            raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±–Ω–æ–≤–∏—Ç—å transformers –¥–æ –≤–µ—Ä—Å–∏–∏ >= 4.40.0. –¢–µ–∫—É—â–∞—è –≤–µ—Ä—Å–∏—è: {transformers_version}")
    else:
        print(f"‚úÖ –í–µ—Ä—Å–∏—è transformers {transformers_version} –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è qwen3")
except Exception as e:
    print(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –ø—Ä–æ–≤–µ—Ä–∫–∏ transformers: {e}")
    raise

from transformers import AutoTokenizer

MODEL_NAME = "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B"

# –û—á–∏—â–∞–µ–º –∫—ç—à transformers –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π (–Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π)
print("üîß –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ transformers...")
try:
    from transformers.utils import TRANSFORMERS_CACHE
    import shutil
    if os.path.exists(TRANSFORMERS_CACHE):
        print(f"   –ö—ç—à –Ω–∞–π–¥–µ–Ω: {TRANSFORMERS_CACHE}")
        # –ù–µ —É–¥–∞–ª—è–µ–º –ø–æ–ª–Ω–æ—Å—Ç—å—é, —Ç–æ–ª—å–∫–æ –¥–ª—è —ç—Ç–æ–π –º–æ–¥–µ–ª–∏
        model_cache = os.path.join(TRANSFORMERS_CACHE, "models--" + MODEL_NAME.replace("/", "--"))
        if os.path.exists(model_cache):
            print(f"   –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –º–æ–¥–µ–ª–∏: {model_cache}")
            shutil.rmtree(model_cache, ignore_errors=True)
except Exception as e:
    print(f"   ‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –æ—á–∏—Å—Ç–∏—Ç—å –∫—ç—à: {e}")

# –ó–∞–≥—Ä—É–∂–∞–µ–º tokenizer —Å trust_remote_code –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ qwen3 –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
print("üîß –ó–∞–≥—Ä—É–∑–∫–∞ tokenizer —Å trust_remote_code=True...")
try:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
    print("‚úÖ Tokenizer –∑–∞–≥—Ä—É–∂–µ–Ω")
except Exception as e:
    print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ tokenizer: {e}")
    print("üí° –ü—Ä–æ–±—É—é –±–µ–∑ trust_remote_code...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=False)
        print("‚ö†Ô∏è  Tokenizer –∑–∞–≥—Ä—É–∂–µ–Ω –ë–ï–ó trust_remote_code (–º–æ–∂–µ—Ç –Ω–µ —Ä–∞–±–æ—Ç–∞—Ç—å –¥–ª—è qwen3)")
    except Exception as e2:
        print(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –∑–∞–≥—Ä—É–∑–∫–∏ tokenizer: {e2}")
        raise

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º vLLM –ü–û–°–õ–ï –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è transformers
print("üîß –ò–º–ø–æ—Ä—Ç vLLM...")
try:
    from vllm import LLM, SamplingParams
    import vllm
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–µ—Ä—Å–∏—é vLLM
    try:
        vllm_version = vllm.__version__
        print(f"   –í–µ—Ä—Å–∏—è vLLM: {vllm_version}")
    except:
        print("   –í–µ—Ä—Å–∏—è vLLM: –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞")
    import uvicorn
    print("‚úÖ vLLM –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω")
except Exception as e:
    print(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –∏–º–ø–æ—Ä—Ç–∞ vLLM: {e}")
    print("üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –æ–±–Ω–æ–≤–∏—Ç—å vLLM: pip install --upgrade vllm")
    raise

# –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
gpu_memory_util = float(os.environ.get("GPU_MEMORY_UTILIZATION", "0.85"))
max_model_length = int(os.environ.get("MAX_MODEL_LEN", "8192"))  # Qwen3 –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –±–æ–ª—å—à–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç

print(f"üîß –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏:")
print(f"   GPU Memory Utilization: {gpu_memory_util}")
print(f"   Max Model Length: {max_model_length}")
print(f"   Trust Remote Code: True")

try:
    print("üîß –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ vLLM...")
    print(f"   –ú–æ–¥–µ–ª—å: {MODEL_NAME}")
    print(f"   trust_remote_code: True")
    print(f"   max_model_len: {max_model_length}")
    print(f"   gpu_memory_utilization: {gpu_memory_util}")
    llm = LLM(
        model=MODEL_NAME,
        tensor_parallel_size=1,
        gpu_memory_utilization=gpu_memory_util,
        max_model_len=max_model_length,
        enforce_eager=False,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∂–∏–º
        trust_remote_code=True,  # –ù–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–ª—è qwen3 –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
    )
    print("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
except ValueError as e:
    error_msg = str(e).lower()
    if "max seq len" in error_msg or "kv cache" in error_msg:
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Å max_model_len={max_model_length}")
        print(f"   –î–µ—Ç–∞–ª–∏: {str(e)[:500]}")
        print("üí° –ü—Ä–æ–±—É—é —É–º–µ–Ω—å—à–∏—Ç—å max_model_len –¥–æ 4096...")
        max_model_length = 4096
        gpu_memory_util = 0.75
        llm = LLM(
            model=MODEL_NAME,
            tensor_parallel_size=1,
            gpu_memory_utilization=gpu_memory_util,
            max_model_len=4096,
            enforce_eager=False,
            trust_remote_code=True,
        )
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Å —É–º–µ–Ω—å—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏!")
    else:
        print(f"‚ùå ValueError –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
        raise
except KeyError as e:
    error_msg = str(e).lower()
    if "qwen3" in error_msg:
        print(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: KeyError –¥–ª—è 'qwen3'")
        print(f"   –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ transformers –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É qwen3")
        print(f"   –¢–µ–∫—É—â–∞—è –≤–µ—Ä—Å–∏—è transformers: {transformers.__version__}")
        print(f"   –¢—Ä–µ–±—É–µ—Ç—Å—è transformers >= 4.40.0")
        print("üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ:")
        print("   1. pip install --upgrade --force-reinstall transformers>=4.40.0")
        print("   2. pip install --upgrade vllm")
        raise RuntimeError(f"transformers –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç qwen3. –í–µ—Ä—Å–∏—è: {transformers.__version__}") from e
    else:
        print(f"‚ùå KeyError –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
        raise
except Exception as e:
    error_msg = str(e).lower()
    if "qwen3" in error_msg or "model type" in error_msg or "architecture" in error_msg:
        print(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ qwen3")
        print(f"   –¢–∏–ø –æ—à–∏–±–∫–∏: {type(e).__name__}")
        print(f"   –°–æ–æ–±—â–µ–Ω–∏–µ: {str(e)[:800]}")
        print(f"   –í–µ—Ä—Å–∏—è transformers: {transformers.__version__}")
        print("üí° –†–µ—à–µ–Ω–∏–µ:")
        print("   1. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ transformers >= 4.40.0")
        print("   2. –û–±–Ω–æ–≤–∏—Ç–µ vLLM: pip install --upgrade vllm")
        print("   3. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ trust_remote_code=True –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è –≤ LLM()")
        raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å qwen3: {e}") from e
    else:
        print(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏:")
        print(f"   –¢–∏–ø: {type(e).__name__}")
        print(f"   –°–æ–æ–±—â–µ–Ω–∏–µ: {str(e)[:800]}")
        raise

app = FastAPI(title="DeepSeek-R1-0528-Qwen3-8B API")


class GenerateRequest(BaseModel):
    prompt: str
    temperature: float = 0.3


@app.post("/generate_deepseek")
async def generate_deepseek(request: GenerateRequest):
    messages = [{"role": "user", "content": request.prompt}]
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º chat template
    prompt_text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )
    
    sampling_params = SamplingParams(
        temperature=request.temperature,
        top_p=0.9,
        top_k=50,
        max_tokens=2048,  # DeepSeek R1 –º–æ–∂–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
    )

    outputs = llm.generate(
        prompt_text,
        use_tqdm=False,
        sampling_params=sampling_params,
    )

    response_text = outputs[0].outputs[0].text.strip()
    return {"response": response_text}


if __name__ == "__main__":
    host = os.environ.get("HOST", "0.0.0.0")
    port = int(os.environ.get("PORT", "8085"))
    uvicorn.run(app, host=host, port=port)

